{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# import required module\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport pandas as pd\nimport re",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-1-438c80e42351>:4: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "def preprocess_text(text):\n    # Remove numeric values\n    text = re.sub(r'\\b\\d+\\b', '', text)\n    # Remove timestamps with various formats\n    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '', text)  # Example: 2023-08-18 (YYYY-MM-DD)\n    text = re.sub(r'\\b\\d{2}/\\d{2}/\\d{4}\\b', '', text)   # Example: 08/18/2023 (MM/DD/YYYY)\n    text = re.sub(r'\\b\\d{2}-\\d{2}-\\d{4}\\b', '', text)   # Example: 08-18-2023 (MM-DD-YYYY)\n    text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text) # Example: 14:30 or 14:30:00\n    text = re.sub(r'\\b\\d{2}:\\d{2}:\\d{2}\\.\\d{1,3}\\b', '', text)  # Example: 14:30:00.123\n    text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text)  # Remove colon-separated times\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "\ndf = pd.read_csv('../data/content_matching.csv',header=None)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "#print(df.head())        # Display the first few rows of the DataFrame\nprint(df.columns)      # List all column names to find the text column\ndf.columns=['text_column']\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Index([0], dtype='int64')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "# Extract text data from the column\ndf['cleaned_text'] = df['text_column'].astype(str).apply(preprocess_text)\n#text_data = df['text_column'].astype(str).tolist()\nprint(df['cleaned_text'])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "0     : When there is a sexual interaction, or there...\n1     Deja Blu & Andre Duqum Interview Sadhguru in t...\n2     : how do we get out of this i'm not enough dra...\n3     : this is why i said that simple process every...\n4     I am advising everyone of you, keep a lamp in ...\n                            ...                        \n80    Sadhguru tells the difference between integrit...\n81    he tells we need not to create a department in...\n82    how to love themselves - To love or hate you n...\n83    I find that there's people that don't have goo...\n84    Do you think that emotional relationship will ...\nName: cleaned_text, Length: 85, dtype: object\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "df.to_csv('../data/cleaned_content.csv', index=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "stop_words_list = list(ENGLISH_STOP_WORDS)\n#keep ngram_range as only 2 as 1 and 2 will give duplicates\ntfidf = TfidfVectorizer(ngram_range=(1, 2),stop_words=stop_words_list)\n\n# Fit and transform the text data\ntfidf_matrix = tfidf.fit_transform(df['cleaned_text'])\n\nfeature_names = tfidf.get_feature_names_out()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": "tfidf_df.to_csv('../data/feature_content.csv', index=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "\n\n\ndef get_significant_ngrams(row, top_n=10):\n    scores = row.to_dict()\n\n    sorted_ngrams = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    return sorted_ngrams\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "def get_significant_words(row, top_n=10):\n    # Get the TF-IDF scores for the document\n    scores = row.to_dict()\n    # Sort the words by score in descending order and get the top_n words\n    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    return sorted_words",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "feature_names = tfidf.get_feature_names_out()\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "#Not in use, Just Trying out\nunigrams = set()\nbigrams = set()\n\n# Identify unigrams and bigrams\nfor feature in feature_names:\n    if len(feature.split()) <= 1:\n        unigrams.add(feature)\n    elif len(feature.split()) == 2:\n        bigrams.add(feature)\n\n# Create a set of unique terms (excluding unigrams that appear in bigrams)\nunique_terms = bigrams.copy()  # Start with bigrams\nfor unigram in unigrams:\n    if all(unigram not in bigram for bigram in bigrams):\n        unique_terms.add(unigram)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "#function to remove duplicates from unigram in bigram\ndef remove_substring_keys(input_dict):\n    # Sort keys by length in descending order\n    sorted_keys = sorted(input_dict.keys(), key=len, reverse=True)\n    \n    # Create a new dictionary to hold the result\n    result_dict = {}\n    \n    # Iterate through sorted keys\n    for key in sorted_keys:\n        # Check if key is not a substring of any key already in the result_dict\n        if not any(key in existing_key for existing_key in result_dict):\n            result_dict[key] = input_dict[key]\n    \n    return result_dict\n\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "'''\nfor i, row in tfidf_df.iterrows():\n    significant_ngrams = get_significant_ngrams(row,6)\n    print(f\"Document {i}:\")\n    for ngram, score in significant_ngrams:\n        if ngram in unique_terms:\n            print(f\"  {ngram}: {score:.4f}\")\n    print(\"\\n\")\n'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nfor i, row in tfidf_df.iterrows():\\n    significant_ngrams = get_significant_ngrams(row,6)\\n    print(f\"Document {i}:\")\\n    for ngram, score in significant_ngrams:\\n        if ngram in unique_terms:\\n            print(f\"  {ngram}: {score:.4f}\")\\n    print(\"\\n\")\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "for i, row in tfidf_df.iterrows():\n    significant_ngrams = get_significant_ngrams(row,8)\n    print(f\"Row {i+1}:\")\n    ngram_dict=dict()\n    for ngram, score in significant_ngrams:\n        ngram_dict[ngram]=f\"{score:.4f}\"\n    # Remove substring keys\n    unique_ngram_dict = remove_substring_keys(ngram_dict)\n    sorted_unique_ngram_dict = dict(sorted(unique_ngram_dict.items(), key=lambda item: item[1], reverse=True))\n\n    #print(ngram_dict)\n    for key,value in sorted_unique_ngram_dict.items():\n        \n        print(f\"  {key}: {value}\")\n    df.at[i,\"Keywords and Scores\"]=str(sorted_unique_ngram_dict)\n    print(\"\\n\")\n    df.to_csv(\"../data/result_keywords.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Row 1:\n  simple possible: 0.1966\n  body memory: 0.1966\n  sexual: 0.1966\n  question: 0.1799\n\n\nRow 2:\n  perception: 0.1905\n  digestive process: 0.1573\n  food remain: 0.1573\n  half hours: 0.1573\n  liquid: 0.1573\n  bag: 0.1573\n\n\nRow 3:\n  thing happened: 0.2609\n  human: 0.2375\n  desire expand: 0.1957\n  expand limitlessly: 0.1305\n\n\nRow 4:\n  engineering: 0.4300\n  stuff: 0.1967\n  trust: 0.1837\n  saying: 0.1584\n  angel comes: 0.1433\n\n\nRow 5:\n  lamp: 0.4174\n  burning: 0.2087\n  light: 0.1909\n  home: 0.1606\n  don want: 0.1538\n  act particular: 0.1044\n  advising: 0.1044\n\n\nRow 6:\n  everybody hurt: 0.3400\n  hurt everybody: 0.1700\n  horse knight: 0.1700\n  hurt perfect: 0.1700\n  human hurt: 0.1700\n\n\nRow 7:\n  somebody make: 0.3754\n  angry somebody: 0.1368\n  freak somebody: 0.1368\n  decide happen: 0.1368\n  choice way: 0.1368\n\n\nRow 8:\n  responsibility: 0.2210\n  doesn matter: 0.1628\n  work: 0.1628\n  match idiots: 0.1473\n  matched: 0.1473\n  stars: 0.1473\n\n\nRow 9:\n  thing ve: 0.2899\n  ve seen: 0.2899\n  infrastructure: 0.1932\n  seen thing: 0.1932\n  great: 0.1768\n  city: 0.1768\n\n\nRow 10:\n  working hell: 0.3835\n  die day: 0.3835\n  hell ll: 0.3835\n  ll die: 0.3509\n\n\nRow 11:\n  muslim: 0.2117\n  baba: 0.2117\n  went: 0.1628\n  atrocity: 0.1411\n  killed: 0.1411\n  legacy: 0.1411\n  hindu: 0.1411\n  men: 0.1411\n\n\nRow 12:\n  love don: 0.2483\n  far: 0.2121\n  fall: 0.2005\n  bigger music: 0.1241\n  bit love: 0.1241\n  bit self: 0.1241\n\n\nRow 13:\n  hours: 0.2243\n  like parrots: 0.1851\n  drink want: 0.1851\n  eat easily: 0.1851\n  make eat: 0.1851\n  banana: 0.1851\n\n\nRow 14:\n  ideal: 0.2380\n  going: 0.1484\n  human: 0.1444\n  believe comes: 0.1190\n  come believe: 0.1190\n  believe ll: 0.1190\n  comes want: 0.1190\n\n\nRow 15:\n  break: 0.2226\n  days: 0.2079\n  puja room: 0.1622\n  dozens: 0.1622\n  month: 0.1622\n  got: 0.1248\n\n\nRow 16:\n  hesitation shame: 0.2092\n  people purify: 0.2092\n  krishna says: 0.2092\n  dance sense: 0.2092\n  laugh dance: 0.2092\n\n\nRow 17:\n  does exist: 0.3732\n  fearful happen: 0.2040\n  exist right: 0.2040\n  fear does: 0.2040\n\n\nRow 18:\n  place: 0.2456\n  live: 0.2206\n  idea heaven: 0.1666\n  crime: 0.1525\n  say: 0.1442\n  making: 0.1424\n\n\nRow 19:\n  obscene: 0.4520\n  love: 0.2607\n  compromising damn: 0.1507\n  distance happens: 0.1507\n  better distance: 0.1507\n  best away: 0.1507\n  away ll: 0.1507\n\n\nRow 20:\n  competition: 0.3741\n  depressed walk: 0.1636\n  feel depressed: 0.1636\n  walk fast: 0.1636\n  faster: 0.1636\n\n\nRow 21:\n  great achievements: 0.1634\n  achievements life: 0.1634\n  build house: 0.1634\n  want create: 0.1634\n  ambition: 0.1634\n  job: 0.1634\n\n\nRow 22:\n  tragedy: 0.3398\n  did happen: 0.2548\n  happened: 0.2143\n  experience alive: 0.1857\n  incidental: 0.1857\n\n\nRow 23:\n  hell happening: 0.2758\n  come: 0.1829\n  isn: 0.1739\n  allow container: 0.1507\n  beautiful means: 0.1507\n  came hell: 0.1507\n\n\nRow 24:\n  know thing: 0.1702\n  love doing: 0.1702\n  doing try: 0.1702\n  try love: 0.1702\n  day: 0.1639\n\n\nRow 25:\n  genius: 0.2750\n  needed: 0.2599\n  flowers limitless: 0.1609\n  limitless way: 0.1609\n  looking life: 0.1609\n  way looking: 0.1609\n  think: 0.1598\n\n\nRow 26:\n  way like: 0.2889\n  thing: 0.1989\n  look carefully: 0.1926\n  like way: 0.1926\n  standards: 0.1762\n\n\nRow 27:\n  devoted doing: 0.2449\n  absolutely devoted: 0.1633\n  legs lungs: 0.1633\n  bolt: 0.1633\n\n\nRow 28:\n  happening: 0.2168\n  everybody shall: 0.1529\n  college matter: 0.1529\n  die hell: 0.1529\n  born ll: 0.1529\n\n\nRow 29:\n  think happen: 0.4213\n  way think: 0.4109\n  thought: 0.2110\n  life happening: 0.1644\n  happening way: 0.1644\n\n\nRow 30:\n  basic ingredient: 0.2001\n  closer grave: 0.2001\n  away hours: 0.2001\n  going away: 0.2001\n  grave came: 0.2001\n  came yes: 0.2001\n\n\nRow 31:\n  just thought: 0.3764\n  negative just: 0.2743\n  identify thought: 0.1371\n  don identify: 0.1371\n  destroys: 0.1371\n\n\nRow 32:\n  stress consequence: 0.3985\n  consequence inability: 0.1993\n  chemistry energy: 0.1993\n  consequence work: 0.1993\n  body thoughts: 0.1993\n\n\nRow 33:\n  prepared certain: 0.1813\n  mangal sutram: 0.1813\n  certain nadi: 0.1813\n  raw: 0.1813\n\n\nRow 34:\n  nature: 0.1807\n  balanced physiologically: 0.1175\n  children involved: 0.1175\n  bangalore dubai: 0.1175\n  boat exposure: 0.1175\n\n\nRow 35:\n  anybody: 0.1947\n  somebody make: 0.1813\n  way: 0.1682\n  cloud number: 0.1321\n  doing wrong: 0.1321\n\n\nRow 36:\n  authority truth: 0.2276\n  truth authority: 0.2276\n  demand respect: 0.1517\n  authority authority: 0.0759\n\n\nRow 37:\n  people interested: 0.2561\n  busy expressing: 0.2561\n  world everybody: 0.2561\n  everybody busy: 0.2561\n  expression: 0.2343\n\n\nRow 38:\n  women: 0.2722\n  people gather: 0.1983\n  capability: 0.1983\n  countries: 0.1983\n  feminine: 0.1983\n  burned: 0.1983\n  million: 0.1814\n\n\nRow 39:\n  trust means: 0.1737\n  trust trust: 0.1737\n  behave: 0.1737\n  expect: 0.1737\n  alright come: 0.0869\n\n\nRow 40:\n  living: 0.1992\n  better somebody: 0.1802\n  driving better: 0.1802\n  wearing better: 0.1802\n  forever: 0.1802\n\n\nRow 41:\n  king: 0.1874\n  nation: 0.1750\n  ordinary people: 0.1365\n  just woman: 0.1365\n  queen: 0.1365\n  puts: 0.1365\n\n\nRow 42:\n  trouble: 0.3016\n  everybody: 0.2063\n  shit: 0.2011\n  invest time: 0.1839\n  time energy: 0.1839\n  world: 0.1830\n\n\nRow 43:\n  life suffering: 0.1719\n  didn say: 0.1719\n  feels: 0.1719\n  married: 0.1573\n  children: 0.1218\n\n\nRow 44:\n  emotions sweet: 0.2636\n  look: 0.2590\n  loving: 0.2411\n  love: 0.2027\n  somebody: 0.1853\n  sweetness: 0.1757\n\n\nRow 45:\n  exaggerate mind: 0.3256\n  unable things: 0.3256\n  somebody: 0.1717\n  dislike: 0.1628\n\n\nRow 46:\n  ashamed: 0.3322\n  thing telling: 0.2215\n  telling live: 0.2215\n  life thing: 0.2215\n  live poor: 0.2215\n  poor life: 0.2215\n\n\nRow 47:\n  disaster disaster: 0.3820\n  problem disaster: 0.3820\n  life problem: 0.3820\n  abiding nation: 0.0000\n\n\nRow 48:\n  genius: 0.3012\n  needed: 0.2847\n  think: 0.1750\n  duty intelligent: 0.1613\n  enjoy life: 0.1613\n  don like: 0.1613\n  extent: 0.1613\n\n\nRow 49:\n  positive thought: 0.2529\n  want remove: 0.2529\n  negative: 0.2313\n  seconds: 0.2313\n  ask just: 0.1264\n\n\nRow 50:\n  logically: 0.2558\n  adding miserable: 0.1279\n  add stress: 0.1279\n  add today: 0.1279\n  add life: 0.1279\n  air: 0.1279\n\n\nRow 51:\n  fly: 0.1991\n  thirty years: 0.1692\n  years age: 0.1580\n  entire life: 0.1233\n  fuel: 0.1233\n\n\nRow 52:\n  control excitement: 0.1837\n  excitement want: 0.1837\n  disaster: 0.1681\n  doing: 0.1302\n  add question: 0.0919\n\n\nRow 53:\n  worked: 0.3012\n  anybody does: 0.2008\n  don anybody: 0.2008\n  pass: 0.1716\n  things: 0.1252\n\n\nRow 54:\n  cell: 0.2446\n  slowly: 0.2090\n  invest: 0.1882\n  body: 0.1619\n  life: 0.1361\n  actually inclusive: 0.1223\n  actually simply: 0.1223\n\n\nRow 55:\n  wait: 0.6093\n  watch: 0.1783\n  think: 0.1770\n  looks: 0.1631\n  ask: 0.1631\n  educated: 0.1523\n  simply: 0.1180\n  people: 0.1020\n\n\nRow 56:\n  phone: 0.3527\n  compulsiveness: 0.1763\n  controlling: 0.1763\n  conscious: 0.1763\n  calls: 0.1763\n  problem: 0.1605\n  years ago: 0.1507\n\n\nRow 57:\n  grace: 0.4561\n  fulfill plans: 0.1955\n  fulfilling: 0.1955\n  moment: 0.1337\n  designed: 0.1303\n\n\nRow 58:\n  joyful: 0.2852\n  process fullest: 0.2078\n  available life: 0.2078\n  life process: 0.2078\n  progressing: 0.2078\n  accounts: 0.2078\n  spirituality: 0.1901\n\n\nRow 59:\n  disappear: 0.1978\n  seeing: 0.1809\n  existence: 0.1690\n  just: 0.1148\n  actually hmm: 0.0989\n  aside prayer: 0.0989\n\n\nRow 60:\n  buddha: 0.2778\n  heaven: 0.2541\n  arrangements: 0.1852\n  let hell: 0.1852\n  gautama: 0.1852\n  heard: 0.1852\n\n\nRow 61:\n  attached: 0.3216\n  devotion: 0.1962\n  mind: 0.1732\n  emotions: 0.1650\n  came: 0.1419\n  love: 0.1237\n  adiyogi: 0.1072\n\n\nRow 62:\n  come form: 0.2403\n  ll come: 0.2403\n  treat: 0.2403\n  property: 0.2198\n  price: 0.2198\n  children: 0.1703\n\n\nRow 63:\n  participants yes: 0.2514\n  yes sadhguru: 0.2514\n  suspense: 0.2514\n  ability enjoy: 0.1676\n\n\nRow 64:\n  body: 0.2599\n  cold water: 0.1963\n  shower: 0.1796\n  head: 0.1677\n  bath: 0.1309\n  dip: 0.1309\n\n\nRow 65:\n  shower: 0.3160\n  trying clean: 0.1727\n  sit program: 0.1727\n  clean body: 0.1727\n  yoga: 0.1476\n  somebody: 0.1366\n\n\nRow 66:\n  alive think: 0.2862\n  thought: 0.2449\n  experiencing: 0.1908\n  bundle: 0.1908\n  life: 0.1769\n  dominant: 0.1745\n\n\nRow 67:\n  economy: 0.3497\n  wanted come: 0.2623\n  richest: 0.2623\n  everybody wanted: 0.1748\n  conquest: 0.1748\n  land: 0.1748\n\n\nRow 68:\n  coming west: 0.2450\n  believe: 0.1979\n  applause trash: 0.1225\n  alive fullest: 0.1225\n  alive isn: 0.1225\n\n\nRow 69:\n  nation: 0.2362\n  fourteen year: 0.2073\n  corruption: 0.2073\n  law: 0.2073\n  year old: 0.1896\n\n\nRow 70:\n  don belong: 0.3332\n  die: 0.1898\n  applause husband: 0.1111\n  belong laughter: 0.1111\n  able digest: 0.1111\n  belong don: 0.1111\n\n\nRow 71:\n  love volume: 0.3688\n  increase love: 0.2459\n  child relationship: 0.1229\n  angry say: 0.1229\n\n\nRow 72:\n  sensitivity: 0.2109\n  yogi: 0.2109\n  eating: 0.1929\n  death: 0.1929\n  food: 0.1929\n  perception: 0.1703\n  ago: 0.1703\n\n\nRow 73:\n  milk drinking: 0.1682\n  child: 0.1294\n  cow milk: 0.1122\n  milk don: 0.1122\n  insane: 0.1122\n\n\nRow 74:\n  breath: 0.3232\n  body: 0.2140\n  distance: 0.1740\n  exhalation: 0.1616\n  inhalation: 0.1616\n  traffic: 0.1479\n  called: 0.1381\n  really: 0.1243\n\n\nRow 75:\n  resources standard: 0.2202\n  isha sacrificied: 0.2202\n  raised white: 0.2202\n  horse tells: 0.2202\n  man raised: 0.2202\n  old man: 0.2202\n\n\nRow 76:\n  knigs questions: 0.2259\n  answers knigs: 0.2259\n  king answers: 0.2259\n  joke year: 0.2259\n  meets: 0.2259\n\n\nRow 77:\n  ujjain: 0.2412\n  maximum: 0.1948\n  construct temples: 0.1206\n  ago secience: 0.1206\n  city temple: 0.1206\n  continues: 0.1206\n\n\nRow 78:\n  happened touring: 0.2425\n  ashram asked: 0.2425\n  asked people: 0.2425\n  branch tree: 0.2425\n  chop branch: 0.2425\n\n\nRow 79:\n  results: 0.3344\n  rage: 0.3344\n  says: 0.2288\n  anger intensity: 0.1672\n  bring instant: 0.1672\n  che guevara: 0.1672\n  come anger: 0.1672\n\n\nRow 80:\n  years moon: 0.3205\n  aone half: 0.3205\n  half inch: 0.3205\n  inch year: 0.3205\n  moon aone: 0.3205\n\n\nRow 81:\n  compromsiedmbut integrity: 0.1809\n  honesty compromsiedmbut: 0.1809\n  difference integrity: 0.1809\n  fairness honesty: 0.1809\n\n\nRow 82:\n  need: 0.2429\n  differtianted foundation: 0.1892\n  company siprutuality: 0.1892\n  department company: 0.1892\n  create department: 0.1892\n  bussiness: 0.1892\n\n\nRow 83:\n  says: 0.2834\n  need: 0.1773\n  different quallity: 0.1381\n  affair wise: 0.1381\n  called love: 0.1381\n  arrangment: 0.1381\n\n\nRow 84:\n  good intention: 0.3197\n  happen sadhguru: 0.1598\n  doesn good: 0.1598\n  does work: 0.1598\n  inlligent: 0.1598\n  don good: 0.1598\n\n\nRow 85:\n  emotional relationship: 0.2057\n  body relationship: 0.2057\n  invest emotions: 0.2057\n  emotions life: 0.2057\n  life powerful: 0.2057\n\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Execute this to find raw re4sults with duplicates of unigram in bigrams\n'''\nfor i, row in tfidf_df.iterrows():\n    significant_ngrams = get_significant_ngrams(row,3)\n    print(f\"Document {i}:\")\n    #ngram_dict=dict()\n    for ngram, score in significant_ngrams:\n        print(f\"  {ngram}: {score:.4f}\")\n    print(\"\\n\")\n'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 54,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nfor i, row in tfidf_df.iterrows():\\n    significant_ngrams = get_significant_ngrams(row,3)\\n    print(f\"Document {i}:\")\\n    #ngram_dict=dict()\\n    for ngram, score in significant_ngrams:\\n        print(f\"  {ngram}: {score:.4f}\")\\n    print(\"\\n\")\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}